# -*- coding: utf-8 -*-
"""LawGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UEX4JMW6O8kriDmJWAutNvTSHTLnoGqb
"""

!pip install -q transformers datasets gradio kaggle sentence-transformers faiss-cpu

from google.colab import files
files.upload()  # Upload kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d anishparkhe0401/indian-laws
!unzip -q indian-laws.zip -d indian_laws

!kaggle datasets download -d omdabral/indian-penal-code-complete-dataset
!unzip -q indian-penal-code-complete-dataset.zip -d ipc_data

import os
import json
import pandas as pd
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import gradio as gr

# Load law sections
with open("/content/indian_laws/combined.json", "r") as f:
    law_data = json.load(f)

# Load IPC dataset
ipc_df = pd.read_csv("/content/ipc_data/FIR_DATASET.csv").dropna()

embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Combine all context texts (law sections + IPC descriptions)
law_texts = [entry["description"] for entry in law_data]
ipc_texts = ipc_df['Description'].tolist()
all_contexts = law_texts + ipc_texts

# Embed all contexts
context_embeddings = embedder.encode(all_contexts, convert_to_tensor=True)
# Initialize embedder

model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large")
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")

qa_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)

def find_semantic_context(question, top_k=3):
    # Encode the question into an embedding
    question_embedding = embedder.encode(question, convert_to_tensor=True)

    # Retrieve top-k most similar contexts
    hits = util.semantic_search(question_embedding, context_embeddings, top_k=top_k)

    # Collect all top-k contexts
    retrieved_contexts = [all_contexts[hit['corpus_id']] for hit in hits[0]]

    # Join them into one string
    combined_context = "\n\n".join(retrieved_contexts)

    return combined_context

def ask_law_bot(question):
    context = find_semantic_context(question, top_k=3)

    prompt = f"""
    You are an Indian legal assistant.

    - If the user asks a **legal question**, answer strictly based on the Constitution of India, IPC, or Indian law sections from the provided context.
    - If the user asks for **suggestions or advice**, provide practical legal guidance along with the relevant law.
    - If the answer is not present in the context, say: "This specific information was not found in the law dataset. Please consult a qualified lawyer for official advice."

    Question: {question}
    Context from laws:
    {context}

    Answer:
    """

    result = qa_pipeline(prompt, max_length=512)[0]["generated_text"]
    return result.strip()

gr.Interface(
    fn=ask_law_bot,
    inputs="text",
    outputs="text",
    title="⚖️ Indian Legal Chatbot (RAG + Generative)",
    description="Ask questions about IPC or Indian law acts. Powered by semantic search and FLAN-T5."
).launch()